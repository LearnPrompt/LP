---
sidebar_position: 25
title: Ten Strategies to Reduce Large Model Hallucinations - GPT Reliability +100
description: This page explores the phenomenon of large model hallucinations in GPT models and provides ten strategies to mitigate them.
keywords: [GPT, AI hallucinations, large models, ChatGPT, AI reliability, machine learning strategies]
slug: /prompt-engineering/reducing-gpt-hallucinations/
---
# 🟡  十大策略减轻大模型整体幻觉 - 这次 GPT 可靠性 +100

>在这个信息爆炸的时代，ChatGPT 如同一艘领航船，带领我们在知识的海洋中航行。它的强大和多功能性，使其成为了我们提高效率的得力助手。但相信你也注意到了，在使用 ChatGPT 时，它有时会详细给你解释一个错误的或者不存在的答案？

这些迷人而又虚幻的答案正是所谓的“大模型幻觉”。就像是在凭空出现的海市蜃楼，它们看似真实，却又难以触及。在我们追求准确信息的路上，这些幻觉有时会误导我们，让我们迷失在虚构与现实的边界。

🔍**这些幻觉究竟是如何形成的？我们应该如何辨别并避免这些误导呢？**

这篇文章我将带你了解什么是大模型幻觉，这些幻觉从何而来，我们又能怎样避免大模型的幻觉呢？

## 什么是大模型的幻觉？

"大模型幻觉"就是当我们用 LLM（比如 chatgpt）回答问题时，它有时会创造出听起来合理但其实是虚构或错误的答案。也就是一本正经的胡说八道。
大模型幻觉看起来会像是这样：
-问题: "2024 年诺贝尔物理学奖是因为什么发现而颁发的？"
-回答（幻觉示例）: "2024 年诺贝尔物理学奖是颁发给了一组科学家，以表彰他们在量子重力波领域的开创性发现。这项发现解决了量子力学与广义相对论之间长期存在的不一致问题，为理解宇宙的基本构造提供了新的视角。这项研究首次实验证明了重力波在量子尺度上的行为，从而开启了新的物理学研究领域。

- 分类：在“A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions”这篇综述中大模型幻觉分为事实性幻觉（*factuality hallucination*）和忠实性幻觉（*faithfulness hallucination*）
  
    ![Hallucination Classification](https://cdn.jsdelivr.net/gh/donttal/imgbed/img/c265845e61ffdd03e54b217672d78bf2.png)
    

## 追根溯源——大模型的幻觉是从何而来？

- 模型产生幻觉的三大来源：**数据源**、**训练过程**和**推理**

如果我们把大模型的训练和推理过程想象成一个**烹饪**的过程：

![Cooking Process](https://cdn.jsdelivr.net/gh/donttal/imgbed/img/39f80f74f7b7fba24ad2194206c96cf8.png)

**数据：食材的质量决定菜品的味道**

最终的菜品（输出内容）的味道（准确性）取决于它使用的食材（数据）。但有时，这些食材可能带有“缺陷”，如错误信息和偏见（包括重复偏见和社会偏见），就像是过期或混杂着变质的食材，会导致最终的菜品不够完美。

**架构缺陷：菜谱设计的重要性**

接下来，考虑一下这道菜的“菜谱”（模型架构）。如果食谱设计得不够好，比如只注重前一个步骤（token）而忽略了整体的配比（上下文关系），那么最后的菜品可能就无法达到理想的口感。这就是架构缺陷，如单向建模和自注意力模块的不足。

**训练过程：学习方式的重要性**

而在“烹饪”过程（训练阶段）中，如果厨师的学习方式存在问题，比如只根据以前的烹饪经验（以往的 token）而不适应新的烹饪环境（新上下文），这就是曝露偏差。同样，如果它在微调时过度迎合顾客（人类偏好），可能会牺牲菜品的真实味道（信息真实性）。

**推理阶段：品尝测试的准确性**

最后，当厨师进行“品尝测试”（推理）时，它可能会受到固有的抽样随机性影响，就像是在盲品时随机猜测味道，这可能导致它生成不准确的内容。同时，由于解码表示不完美，比如过度关注相邻文本（上下文关注不足）和 softmax 瓶颈（输出概率分布的表达能力受限），它的品尝判断可能并不总是准确的。

根据模型产生幻觉的来源来分析：如果我们如果想要根本性的解决大模型的幻觉问题，从提示词远远不够的。

## 如何解决大语言模型的幻觉问题？

### 提示工程部分：

### **提供引用信息：确保信息的准确性**

通过向模型提供或引用相关的文档和上下文信息，确保其产生的回答基于可靠的数据源。**例如，当提出一个问题时，可以附上相关的文章链接或数据报告，要求模型基于这些具体信息来生成答案。**

![](https://cdn.jsdelivr.net/gh/donttal/imgbed/img/4f5d99ce9297f6436567f210f0b20d29.png)

### **构建高效的提示词模板：引导内容生成**

使用定制的提示词模板，包括明确的指令、用户输入、输出要求和相关样例，以引导模型产生期望的回答。**例如，设计模板时可以先明确问题背景，然后指出所需答案的类型，最后给出一个类似问题的解答示例。**

下面是根据我在”提示词“群中分享的 CREATE 模板构建的 prompt：

```
您是一位经验丰富的翻译专家，擅长将文本从英语翻译成中文。
请将下面的英文段落翻译成中文。
[在此处插入一小段英文文本]
在翻译时，请注意保持原文的语气和文化背景，同时确保中文表达流畅自然。
提供一段流畅、准确的中文翻译文本。
如果原文中包含特定的术语或文化独特之处，请在翻译旁边简要解释。

```

### **应用思维链（CoT）和定制化提示：提高推理能力**

利用上下文导向推理（CoT）方法和定制化的提示词，引导模型进行详细的逐步推理，并提供具体的示例来帮助模型理解预期的输出格式。例如，要求模型在回答问题时先展示其思考过程的各个步骤，再给出最终结论。

[我们网站上的教程](https://www.learnprompt.pro/article/promptCOT)

![Chain of Thought](https://cdn.jsdelivr.net/gh/donttal/imgbed/img/70ea8b24c0367ebdd8a3d8e2096420fa.png)

### **任务分解与递归链接：简化复杂问题并创建连贯的思维链**

将复杂的问题分解为简单、模块化的步骤，并通过递归链接的方式，将一个子任务的输出作为下一个子任务的输入。这样做可以确保答案的连贯性和逻辑性。**例如，分析市场趋势并预测未来变化的问题，可以分解为分析当前市场趋势和基于当前趋势预测未来变化两个子任务。**

（*下面这一部分有点门槛，没有基础的朋友可以跳过到大模型思考部分*）

### **5. 先进的提示工程技术**

- 通过检索增强生成（RAG）：结合不同阶段的检索和生成过程，如 LLM-Augmenter、知识检索、D&Q 框架、RARR 和原始的 RAG 模型，以提升文本的准确性和相关性。

![RAG Techniques](https://cdn.jsdelivr.net/gh/donttal/imgbed/img/c2422a4fac827544dae3e76591b86653.png)

- 通过反馈和推理进行自我完善：包括用户反馈迭代改进（如“Prompting GPT-3 for Reliability”）、发现并缓解自我矛盾（如 ChatProtect）以及通过反馈循环进行交互式改进（如自我反思方法论）。

### 训练相关的幻觉解决策略

1. **完善预训练策略**

最新研究表明，通过改进预训练策略，确保模型有更丰富的上下文理解和规避偏见，可以有效应对幻觉问题。这包括让模型更好地理解文档式的非结构化事实知识，防止理解碎片化和关联性缺失。

1. **增强模型对事实的关联理解**

例如，有研究在文档的每个句子后附加一个 TOPIC PREFIX，将它们转换为独立的事实，从而增强模型对事实关联的理解。这种方法有助于模型更准确地理解和关联信息。

1. **改进人类偏好判断和激活引导**

通过改善模型在模仿人类偏好判断时的表现，以及激活引导技术，可以减轻对齐错位问题，提高模型输出的准确性和相关性。

### 推理相关的幻觉解决策略

1. **事实增强解码与译后编辑解码**

研究人员探讨了包括事实增强解码和译后编辑解码在内的高级策略，以减少模型输出与原始上下文的偏离。

1. **忠实度增强解码**

这一策略强调与用户提供的上下文保持一致，并增强生成内容的一致性。忠实度增强解码可以分为两类：

- 上下文一致性：例如上下文感知解码（CAD），它通过调整输出分布，减少对先验知识的依赖，促进模型对上下文信息的关注。
- 逻辑一致性：包括使用知识蒸馏框架来增强思维链提示中固有的自洽性，确保模型推理的逻辑连贯性。

（大家可以就想要了解点去搜索详细的知识，知识过于庞大，这就就只给大家提供思路）

---

## 关于大模型幻觉的思考：

大模型的幻觉问题一直困扰着使用大模型的我们，同时也困扰着 LLM 科研工作者，大家都希望能通过一些技术手段解决大模型的幻觉问题，这也是 LLM 一个绕不过去的课题。就当我们还在抱怨着大模型幻觉的一切时，OpenAI 科学家 **Andrej Karpathy** 对大模型幻觉的观点，却让我眼前一亮！

![Andrej Karpathy](https://cdn.jsdelivr.net/gh/donttal/imgbed/img/a9552c83d4bf1f7697cebd9226a23fd7.png)

**Andrej Karpathy 表示：**

> ”因为在某种意义上，幻觉正是 LLMs 所做的一切。它们是造梦机。
我们通过提示词引导它们的梦境。提示词启动了梦境，基于 LLM 对训练文档的模糊回忆，大多数时候结果会走向有用的地方。
只有当梦境进入被认为事实上不正确的领域时，我们才将其标记为“幻觉”。这看起来像是一个 bug，但它只是 LLM 一直在做的事情。“
> 

他认为大模型就像一个造梦机，幻觉是大模型的特点，只是当梦境进入被认为事实上不正确的领域时，我们才将其标记为“幻觉”。

> ”如果考虑到另一个极端，比如搜索引擎。它接受提示词，然后逐字返回其数据库中最相似的“训练文档”之一。你可以说这个搜索引擎有一个“创造力问题”——它永远不会回应一些新的东西。一个 LLM100% 在做梦，有幻觉问题。搜索引擎 0% 在做梦，有创造力问题。
尽管如此，我意识到人们_实际上_指的是他们不希望 LLM 助手（像 ChatGPT 这样的产品）产生幻觉。LLM 助手是一个比 LLM 本身复杂得多的系统，即使 LLM 是其核心。在这些系统中减少幻觉的方法有很多——使用检索增强生成（RAG）通过上下文学习更牢固地将梦境锚定在真实数据中，可能是最常见的方法之一。多重样本间的不一致、反思、验证链。从激活中解码不确定性。工具使用。所有这些都是活跃且非常有趣的研究领域。
长话短说，我知道我在说得非常啰嗦，但 LLM 没有“幻觉问题”。幻觉不是 bug，而是 LLM 最大的特点。LLM 助手有幻觉问题，我们应该解决它。“
> 

他还将 LLM 与搜索引擎做对比，虽然是比较极端的例子，但是也同时说明某种程度上幻觉是 LLM 创造力的来源。他认为 chatGPT 不能代表 LLM，而是属于 LLM 助手，他的幻觉问题应该得到解决。

# 写在最后

”梦“和 AI，在某种程度上，拥有着神秘且未知的共鸣。它们既蕴藏着无限的想象空间，又笼罩在未知的面纱之下。可能正是这种神秘感和不可预测性，为人类的世界带来了意想不到的奇妙和美好。它们的存在，仿佛是宇宙在向我们展示的，超越想象的绚烂多彩。

💡 有关大模型幻觉上的问题，欢迎您在底部评论区留言，一起交流~
